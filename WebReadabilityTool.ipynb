{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGIS 501 Final Project - Web Scraping and Natural Language Processing\n",
    "\n",
    "In this project file, I use lxml, BeautifulSoup, Pandas, and textstat libraries to pull a list of all available page URLs in the domain www.tpchd.org, and then parse through the main content area of each and evaluate the reading level.  The end product is a csv file that contains the URL and reading level of all 330+ pages. My boss is gonna owe me big time.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************************************************************************************************************************************************************************************************************************************Assessment of page readability is complete.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "import requests                # for getting the XML sitemap from tpchd.org\n",
    "from requests import get       # for calling HTML objects from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas                  # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam server requests (can slow rate)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "\n",
    "# Open sitemap showing all pages from URL to xml\n",
    "sitereq = requests.request('GET', 'http://www.tpchd.org/sitemap-page-1.xml')\n",
    "sitemapxml = sitereq.text\n",
    "sitemapxml = sitemapxml.replace('ï»¿<?xml version=\"1.0\" encoding=\"UTF-8\"?><urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">', \"<urlset>\")\n",
    "# I had to do a replace because the xml header I was getting was throwing off the etree parser.\n",
    "# This scrubs some wierd, non-utf8 chars and simplifies the parent directory name.\n",
    "\n",
    "# Write xml to file\n",
    "f = open('sitemap.xml', 'w')\n",
    "f.write(sitemapxml)\n",
    "f.close()\n",
    "\n",
    "# Next, I create an xml ElementTree so I can easily iterate through.\n",
    "tree = etree.parse('sitemap.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# this will do the iterating, and will only save the URL to a list, the rest of the xml is not needed.   \n",
    "# The xml structure was a little wonky, so I append URL by element, not by iterating through <loc> tags.\n",
    "urllist = []\n",
    "score = []\n",
    "for i in range(len(root)):\n",
    "    urlname = root[i][0].text\n",
    "    urllist.append(urlname)\n",
    "\n",
    "#print (urllist)\n",
    "\n",
    "# Now, we iterate through URL list, request each HTML with get(), pass HTML to BeautifulSoup object.\n",
    "# Once we have the BS object, search find all <div> elements with the content class we are looking for (main text).\n",
    "# sleep() inserts a 1 to 3 second pause between requests to avoid the server shutting me out for spamming.\n",
    "for things in urllist:\n",
    "    #sleep(randint(1,3))   \n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \")\n",
    "    print(\"*\", end=\"\", flush=True) #makeshift status bar.\n",
    "    \n",
    "    # then, we need to test if the page had a target <div> or not, otherwise we get empty outputs in the csv\n",
    "    if(len(test) == 0):\n",
    "        score.append('No content')\n",
    "    \n",
    "    # if it does have the div, we need to concatenate the text to a new list to drop the style tags\n",
    "    else:\n",
    "        pgtext = []\n",
    "        for stuff in test:\n",
    "            pgtext.append(stuff.text)\n",
    "        \n",
    "        # then join the list into a text string to run it through the textstat module for flesch-kincaid level\n",
    "        txtstr = \" \".join(pgtext)\n",
    "        \n",
    "        if(textstat.lexicon_count(txtstr)==0):\n",
    "            score.append('No content')\n",
    "        else:\n",
    "            \n",
    "            # catching other text string errors\n",
    "            while True:\n",
    "                try:\n",
    "                    rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "                    score.append(rdscore)\n",
    "                    break\n",
    "                except TypeError:\n",
    "                    score.append('Content not text')\n",
    "                #except ZeroDivisionError:               <-----not passing as ZeroDivisionError, wtf!?\n",
    "                    #score.append('Content empty')\n",
    "\n",
    "# Next is a Pandas dataframe to organize our URL list and reading scores into a table.\n",
    "siteinventory = pandas.DataFrame(list(zip(urllist, score)), columns = ['URL', 'FK Reading Level'])\n",
    "\n",
    "# And then.... Save DataFrame to CSV.  Thats it!\n",
    "siteinventory.to_csv('siteinventory.csv')\n",
    "print ('Assessment of page readability is complete.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test box for finding readability\n",
    "\n",
    "I used a test code snippet so I didn't have to run through 300+ pages to troubleshoot the readability calculation module [textstat](https://pypi.python.org/pypi.textstat).  The URLs I tested were intentional, one was a control without a defined content area-the other was a complex page with content areas broken up into two separate divs that used H3, H2, P text tags and a table list (< tl >). All content showed up in the test print, so I am confident that multiple content divs will not break the function.\n",
    "\n",
    "BeautifulSoup was a giant pain in the ass to figure out, but it's pretty slick once you get the hang of it.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Content', 7.4]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "from requests import get       # for getting sitemap xml, and calling HTML object from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas as pd            # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam the site with requests (can slow requests)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "url = ['https://www.tpchd.org/','https://www.tpchd.org/healthy-people/antibiotic-awareness']\n",
    "score = []\n",
    "\n",
    "# Iterate through URL list, request each HTML with get(), pass HTML to BeautifulSoup object\n",
    "# Once we have the BS object, search find all <div> elements with the content class we are looking for (main text)\n",
    "for things in url:\n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \")\n",
    "    \n",
    "    # then, we need to test if the page had a target <div> or not, otherwise we get errors.\n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    \n",
    "    # if it does have the div, we need to concatenate the text to a new list to drop the style tags\n",
    "    else:\n",
    "        pgtext = []\n",
    "        for stuff in test:\n",
    "            pgtext.append(stuff.text)\n",
    "        \n",
    "        # then join the list into a text string and run it through the textstat module for flesch-kincaid level\n",
    "        txtstr = \" \".join(pgtext)\n",
    "        rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "        score.append(rdscore)\n",
    "        \n",
    "print(score)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FinalProj]",
   "language": "python",
   "name": "conda-env-FinalProj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
